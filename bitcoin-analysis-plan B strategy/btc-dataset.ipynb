{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python38132bitd3bc09b3c8974684859eb2836ff2aae1",
   "display_name": "Python 3.8.1 32-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intro\n",
    "The purpose of this work is to extract a dataset of all bitcoin blocks as well as bitcoin historic prices.\n",
    "\n",
    "There are several ways to get block information, e.g.:\n",
    "\n",
    "- Number of blocks per month can be directly queried from the bitcoin blockchain with Python/RPC/bitcoind https://github.com/jgarzik/python-bitcoinrpc\n",
    "- For that we need a Bicoin full node, so we load the CSV file from the GitHub page\n",
    "- Here we can select which data from the blockchain extract https://blockchair.com/bitcoin/blocks\n",
    "- Some further info: https://bitcoin.stackexchange.com/questions/73186/csv-file-of-every-block-timestamp-in-btc-history\n",
    "\n",
    "\n",
    "Concerning bitcoin prices, I found this: \n",
    "- https://www.reddit.com/r/algotrading/comments/b543yn/made_a_webapp_to_get_price_and_indicator_as_csv/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download blocks from Blockchair\n",
    "\n",
    "I tried to follow the export instructions by Marcel Burger here: https://medium.com/burgercrypto-com/building-a-bitcoin-dataset-b2f526d667ce\n",
    "\n",
    "Unfortunately, as of February 23, 2020, I cannot find the \"Export\" function that Marcel refers to, so I am extracting the block data from the dumps offered by blockchair here: https://gz.blockchair.com/bitcoin/blocks/\n",
    "It is very cumbersome, but eventually we get one CSV file with one line per block. Pretty cool, I should to say.\n",
    "\n",
    "*Warning*: The following code will download a big amount of data to your computer (ca. 400MB), so make sure you have enough space (and patience) for the download.\n",
    "The good thing is that you need to do this process only once and then maybe update your dataset incrementally , but adjusting the start and end dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets install first all modules used elsewhere in this code\n",
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import gzip\n",
    "import shutil\n",
    "from datetime import timedelta, date\n",
    "\n",
    "\n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int ((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "\n",
    "def unpack(filename_gz, filename_unpacked):\n",
    "    try:\n",
    "        with gzip.open(filename_gz, 'rb') as f_in:\n",
    "            with open(filename_unpacked, 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "    except:\n",
    "        print(\"ERROR - Couldn't unpack the file \" + filename_gz)\n",
    "\n",
    "\n",
    "BLOCKS_DOWNLOAD_DIR = './blocks/'\n",
    "START_DATE = date(2016, 1, 1) # originally: 20090103\n",
    "END_DATE = date(2020, 2, 23) # 20220223\n",
    "FILENAME_PREFIX = 'blockchair_bitcoin_blocks_'\n",
    "FILENAME_SUFIX = '.tsv.gz'\n",
    "URL_PREFIX = 'https://gz.blockchair.com/bitcoin/blocks/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for single_date in daterange(START_DATE, END_DATE):\n",
    "    filename = FILENAME_PREFIX + single_date.strftime(\"%Y%m%d\") + FILENAME_SUFIX\n",
    "    url = URL_PREFIX + filename\n",
    "    print(\"Downloading: \" + url)\n",
    "    try:\n",
    "        urllib.request.urlretrieve (url, BLOCKS_DOWNLOAD_DIR + filename)\n",
    "        print(\"File downloaded: \" + url)\n",
    "        print(\"--> Unpacking file ...\")\n",
    "        unpack(BLOCKS_DOWNLOAD_DIR + filename, BLOCKS_DOWNLOAD_DIR + FILENAME_PREFIX + single_date.strftime(\"%Y%m%d\") + '.tsv')\n",
    "        print(\"--> File unpacked\")\n",
    "    except IOError:\n",
    "        print(\"ERROR - Couldn't download file: \" + url)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge all individual TSV files into one CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "ALL_BLOCKS_TSV = 'all_blocks.tsv'\n",
    "ALL_BLOCKS_CSV = 'all_blocks.csv'\n",
    "START_DATE = date(2009, 1, 3) # originally: 20090103\n",
    "START_DATE_PLUS_ONE = date(2009, 1, 4) \n",
    "END_DATE = date(2020, 2, 23) # 20220223 # The end date is not inclusive in the range iteration,\n",
    "\n",
    "with open(BLOCKS_DOWNLOAD_DIR + ALL_BLOCKS_TSV, \"w\") as fout:\n",
    "    # first file:\n",
    "    fin = BLOCKS_DOWNLOAD_DIR + FILENAME_PREFIX + START_DATE.strftime(\"%Y%m%d\") + '.tsv'\n",
    "    try:\n",
    "        print('Trying to create headers and initial contents from file ' + fin)\n",
    "        for line in open(fin):\n",
    "            fout.write(line)\n",
    "    except:\n",
    "        print(\"Problem with file \" + fin)\n",
    "\n",
    "with open(BLOCKS_DOWNLOAD_DIR + ALL_BLOCKS_TSV, \"a\") as fout:\n",
    "    # now the rest:    \n",
    "    \n",
    "    for single_date in daterange(START_DATE_PLUS_ONE, END_DATE):\n",
    "        \n",
    "        fin = BLOCKS_DOWNLOAD_DIR + FILENAME_PREFIX + single_date.strftime(\"%Y%m%d\") + '.tsv'\n",
    "        try:\n",
    "            print('Trying to append contents of file ' + fin)\n",
    "            f = open(fin)\n",
    "            f.readline() # skip the header\n",
    "            fout.write('\\n')\n",
    "            for line in f:\n",
    "                fout.write(line)\n",
    "            f.close() # not really needed\n",
    "            print('... Success!')\n",
    "        except IOError:\n",
    "            print(\"... Problem with file \" + fin)\n",
    "\n",
    "    \n",
    "# Convert to CSV\n",
    "# read tab-delimited file\n",
    "with open(BLOCKS_DOWNLOAD_DIR + ALL_BLOCKS_TSV,'r') as fin:\n",
    "    cr = csv.reader(fin, delimiter='\\t')\n",
    "    filecontents = [line for line in cr]\n",
    "\n",
    "# write comma-delimited file (comma is the default delimiter)\n",
    "with open(BLOCKS_DOWNLOAD_DIR + ALL_BLOCKS_CSV,'w', newline='') as fout:\n",
    "    cw = csv.writer(fout, quotechar='', quoting=csv.QUOTE_NONE, escapechar='\\\\')\n",
    "    cw.writerows(filecontents)\n",
    "    \n",
    "print('COOL! CSV file with all blocks created in ' + BLOCKS_DOWNLOAD_DIR + ALL_BLOCKS_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean up\n",
    "If it makes sense to you, run the following lines to delete the individual files, both .tsv and .tsv.gz\n",
    "\n",
    "*Note*: If you feel more comfortable deleting the files manually from your file explorer, go ahead, that's easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# for p in Path(BLOCKS_DOWNLOAD_DIR).glob(FILENAME_PREFIX + \"*\"):\n",
    "  #  p.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download bitcoin prices\n",
    "\n",
    "Blockchain.com offers a CSV download of the desired date range. The only drawback is that the price data points are every 3 days.\n",
    "\n",
    "https://www.blockchain.com/charts/market-price?timespan=all\n",
    "\n",
    "Here is the CSV download:\n",
    "\n",
    "https://api.blockchain.info/charts/market-price?timespan=all&format=csv\n",
    "\n",
    "\n",
    "* Also here, I tried to follow the export instructions by Marcel Burger: https://medium.com/burgercrypto-com/building-a-bitcoin-dataset-b2f526d667ce\n",
    "However the links in the Reddit post mentioned are not working anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}